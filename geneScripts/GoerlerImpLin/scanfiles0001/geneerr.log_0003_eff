using GENE executable in bin /home/stirkas/Workspace/GENE/genecode/prob05
gmake -f ../makefiles/rules.mk run
gmake[1]: Entering directory '/home/stirkas/Workspace/GENE/genecode/prob05'
Calling GENE with 24 MPI and 1 OpenMP thread(s)
 MPI_THREAD_MULTIPLE initialized.

*****************************************************
***** This is GENE 11 (release 1.8 - patch 1) *******
***** GIT branch hash: 41ff07b                *******
***** GIT master hash:                        *******
*****************************************************

WARNING: no dpdx_term treatment defined!!
         Setting dpdx_term = 'gradB_eq_curv' ... 
no which_ev given in parameters, using jd
decreasing nx0 by 1 to have all kx physical for full matrix computations
WARNING: no dpdx_term treatment defined!!
         Setting dpdx_term = 'gradB_eq_curv' ... 
no which_ev given in parameters, using jd
decreasing nx0 by 1 to have all kx physical for full matrix computations
WARNING: no dpdx_term treatment defined!!
         Setting dpdx_term = 'gradB_eq_curv' ... 
no which_ev given in parameters, using jd
decreasing nx0 by 1 to have all kx physical for full matrix computations
WARNING: no dpdx_term treatment defined!!
         Setting dpdx_term = 'gradB_eq_curv' ... 
no which_ev given in parameters, using jd
decreasing nx0 by 1 to have all kx physical for full matrix computations
WARNING: no dpdx_term treatment defined!!
         Setting dpdx_term = 'gradB_eq_curv' ... 
WARNING: no dpdx_term treatment defined!!
         Setting dpdx_term = 'gradB_eq_curv' ... 
WARNING: no dpdx_term treatment defined!!
         Setting dpdx_term = 'gradB_eq_curv' ... 
no which_ev given in parameters, using jd
decreasing nx0 by 1 to have all kx physical for full matrix computations
no which_ev given in parameters, using jd
decreasing nx0 by 1 to have all kx physical for full matrix computations
WARNING: no dpdx_term treatment defined!!
         Setting dpdx_term = 'gradB_eq_curv' ... 
WARNING: no dpdx_term treatment defined!!
         Setting dpdx_term = 'gradB_eq_curv' ... 
no which_ev given in parameters, using jd
decreasing nx0 by 1 to have all kx physical for full matrix computations
no which_ev given in parameters, using jd
decreasing nx0 by 1 to have all kx physical for full matrix computations
WARNING: no dpdx_term treatment defined!!
         Setting dpdx_term = 'gradB_eq_curv' ... 
no which_ev given in parameters, using jd
decreasing nx0 by 1 to have all kx physical for full matrix computations
no which_ev given in parameters, using jd
decreasing nx0 by 1 to have all kx physical for full matrix computations
Using a maximum of  1300.00 MB per core.
adapted autoparallelization ranges to optimize preconditioner
Using a maximum of  1300.00 MB per core.
adapted autoparallelization ranges to optimize preconditioner
Using a maximum of  1300.00 MB per core.
adapted autoparallelization ranges to optimize preconditioner
Using a maximum of  1300.00 MB per core.
adapted autoparallelization ranges to optimize preconditioner
Using a maximum of  1300.00 MB per core.
adapted autoparallelization ranges to optimize preconditioner
Using a maximum of  1300.00 MB per core.
adapted autoparallelization ranges to optimize preconditioner
Using a maximum of  1300.00 MB per core.
adapted autoparallelization ranges to optimize preconditioner
Using a maximum of  1300.00 MB per core.
adapted autoparallelization ranges to optimize preconditioner
Using a maximum of  1300.00 MB per core.
adapted autoparallelization ranges to optimize preconditioner
WARNING: no dpdx_term treatment defined!!
         Setting dpdx_term = 'gradB_eq_curv' ... 
no which_ev given in parameters, using jd
decreasing nx0 by 1 to have all kx physical for full matrix computations
WARNING: no dpdx_term treatment defined!!
         Setting dpdx_term = 'gradB_eq_curv' ... 
no which_ev given in parameters, using jd
decreasing nx0 by 1 to have all kx physical for full matrix computations
Using a maximum of  1300.00 MB per core.
adapted autoparallelization ranges to optimize preconditioner
Using a maximum of  1300.00 MB per core.
adapted autoparallelization ranges to optimize preconditioner
Using a maximum of  1300.00 MB per core.
adapted autoparallelization ranges to optimize preconditioner
N for parallel boundary condition: 1
double precision computation
parallel direction: 4th order Arakawa scheme
=========== hyper diffusion ============
hyp_z =      2.000
hyp_v =      0.200

recurrence times for the different species:
ions       :   22.95490
protons    :   72.58975
Electrons are adiabatic, beta is set to zero
*************************************************************
Starting eigenvalue computation with PETSC-3.7.7/SLEPc-3.7.4
*************************************************************

initialization: alm                 
  all kx, ky modes equally excited,
  jacobian to the power of  2.000E+00 in z

EV solver/preconditioner: jd / asm
using    0 test vectors with length    253952, i.e.    0.00MB
precision:   0.1000E-03
--------------------------------------------------------------

Creating explicit representation of the linear operator without the field part
Rank of the linear operator:   253952
Memory requirements:    25.19 MB per core
N for parallel boundary condition: 1
double precision computation
parallel direction: 4th order Arakawa scheme
=========== hyper diffusion ============
hyp_z =      2.000
hyp_v =      0.200

recurrence times for the different species:
ions       :   22.95490
protons    :   72.58975
Electrons are adiabatic, beta is set to zero
*************************************************************
Starting eigenvalue computation with PETSC-3.7.7/SLEPc-3.7.4
*************************************************************

initialization: alm                 
  all kx, ky modes equally excited,
  jacobian to the power of  2.000E+00 in z

EV solver/preconditioner: jd / asm
using    0 test vectors with length    253952, i.e.    0.00MB
precision:   0.1000E-03
--------------------------------------------------------------

Creating explicit representation of the linear operator without the field part
Rank of the linear operator:   253952
Memory requirements:    25.19 MB per core
N for parallel boundary condition: 1
double precision computation
parallel direction: 4th order Arakawa scheme
=========== hyper diffusion ============
hyp_z =      2.000
hyp_v =      0.200

recurrence times for the different species:
ions       :   22.95490
protons    :   72.58975
Electrons are adiabatic, beta is set to zero
*************************************************************
Starting eigenvalue computation with PETSC-3.7.7/SLEPc-3.7.4
*************************************************************

initialization: alm                 
  all kx, ky modes equally excited,
  jacobian to the power of  2.000E+00 in z

EV solver/preconditioner: jd / asm
using    0 test vectors with length    253952, i.e.    0.00MB
precision:   0.1000E-03
--------------------------------------------------------------

Creating explicit representation of the linear operator without the field part
Rank of the linear operator:   253952
Memory requirements:    25.19 MB per core
N for parallel boundary condition: 1
double precision computation
parallel direction: 4th order Arakawa scheme
=========== hyper diffusion ============
hyp_z =      2.000
hyp_v =      0.200

recurrence times for the different species:
ions       :   22.95490
protons    :   72.58975
Electrons are adiabatic, beta is set to zero
*************************************************************
Starting eigenvalue computation with PETSC-3.7.7/SLEPc-3.7.4
*************************************************************

initialization: alm                 
  all kx, ky modes equally excited,
  jacobian to the power of  2.000E+00 in z

EV solver/preconditioner: jd / asm
using    0 test vectors with length    253952, i.e.    0.00MB
precision:   0.1000E-03
--------------------------------------------------------------

Creating explicit representation of the linear operator without the field part
Rank of the linear operator:   253952
Memory requirements:    25.19 MB per core
N for parallel boundary condition: 1
double precision computation
parallel direction: 4th order Arakawa scheme
=========== hyper diffusion ============
hyp_z =      2.000
hyp_v =      0.200

recurrence times for the different species:
ions       :   22.95490
protons    :   72.58975
Electrons are adiabatic, beta is set to zero
*************************************************************
Starting eigenvalue computation with PETSC-3.7.7/SLEPc-3.7.4
*************************************************************
N for parallel boundary condition: 1
double precision computation
parallel direction: 4th order Arakawa scheme
=========== hyper diffusion ============
hyp_z =      2.000
hyp_v =      0.200

recurrence times for the different species:
ions       :   22.95490
protons    :   72.58975
Electrons are adiabatic, beta is set to zero

initialization: alm                 
  all kx, ky modes equally excited,
  jacobian to the power of  2.000E+00 in z

EV solver/preconditioner: jd / asm
using    0 test vectors with length    253952, i.e.    0.00MB
precision:   0.1000E-03
--------------------------------------------------------------

Creating explicit representation of the linear operator without the field part
Rank of the linear operator:   253952
Memory requirements:    25.19 MB per core
*************************************************************
Starting eigenvalue computation with PETSC-3.7.7/SLEPc-3.7.4
*************************************************************

initialization: alm                 
  all kx, ky modes equally excited,
  jacobian to the power of  2.000E+00 in z

EV solver/preconditioner: jd / asm
using    0 test vectors with length    253952, i.e.    0.00MB
precision:   0.1000E-03
--------------------------------------------------------------

Creating explicit representation of the linear operator without the field part
Rank of the linear operator:   253952
Memory requirements:    25.19 MB per core
Time to build explicit L_g:     2.10

--------------------------------------------------------------

using additive Schwarz preconditioning with   8 blocks per processor
N for parallel boundary condition: 1
double precision computation
parallel direction: 4th order Arakawa scheme
=========== hyper diffusion ============
hyp_z =      2.000
hyp_v =      0.200

recurrence times for the different species:
ions       :   22.95490
protons    :   72.58975
Electrons are adiabatic, beta is set to zero
*************************************************************
Starting eigenvalue computation with PETSC-3.7.7/SLEPc-3.7.4
*************************************************************

initialization: alm                 
  all kx, ky modes equally excited,
  jacobian to the power of  2.000E+00 in z

EV solver/preconditioner: jd / asm
using    0 test vectors with length    253952, i.e.    0.00MB
precision:   0.1000E-03
--------------------------------------------------------------

Creating explicit representation of the linear operator without the field part
Rank of the linear operator:   253952
Memory requirements:    25.19 MB per core
N for parallel boundary condition: 1
double precision computation
parallel direction: 4th order Arakawa scheme
=========== hyper diffusion ============
hyp_z =      2.000
hyp_v =      0.200

recurrence times for the different species:
ions       :   22.95490
protons    :   72.58975
Electrons are adiabatic, beta is set to zero
*************************************************************
Starting eigenvalue computation with PETSC-3.7.7/SLEPc-3.7.4
*************************************************************
N for parallel boundary condition: 1
double precision computation
parallel direction: 4th order Arakawa scheme
=========== hyper diffusion ============
hyp_z =      2.000
hyp_v =      0.200

recurrence times for the different species:
ions       :   22.95490
protons    :   72.58975
Electrons are adiabatic, beta is set to zero

initialization: alm                 
  all kx, ky modes equally excited,
  jacobian to the power of  2.000E+00 in z

EV solver/preconditioner: jd / asm
using    0 test vectors with length    253952, i.e.    0.00MB
precision:   0.1000E-03
*************************************************************
Starting eigenvalue computation with PETSC-3.7.7/SLEPc-3.7.4
*************************************************************
--------------------------------------------------------------

Creating explicit representation of the linear operator without the field part
Rank of the linear operator:   253952
Memory requirements:    25.19 MB per core
N for parallel boundary condition: 1
double precision computation
parallel direction: 4th order Arakawa scheme
=========== hyper diffusion ============
hyp_z =      2.000
hyp_v =      0.200

recurrence times for the different species:
ions       :   22.95490
protons    :   72.58975
Electrons are adiabatic, beta is set to zero

initialization: alm                 
  all kx, ky modes equally excited,
  jacobian to the power of  2.000E+00 in z

EV solver/preconditioner: jd / asm
using    0 test vectors with length    253952, i.e.    0.00MB
precision:   0.1000E-03
--------------------------------------------------------------

Creating explicit representation of the linear operator without the field part
Rank of the linear operator:   253952
Memory requirements:    25.19 MB per core
*************************************************************
Starting eigenvalue computation with PETSC-3.7.7/SLEPc-3.7.4
*************************************************************

initialization: alm                 
  all kx, ky modes equally excited,
  jacobian to the power of  2.000E+00 in z

EV solver/preconditioner: jd / asm
using    0 test vectors with length    253952, i.e.    0.00MB
precision:   0.1000E-03
--------------------------------------------------------------

Creating explicit representation of the linear operator without the field part
Rank of the linear operator:   253952
Memory requirements:    25.19 MB per core
N for parallel boundary condition: 1
double precision computation
parallel direction: 4th order Arakawa scheme
=========== hyper diffusion ============
hyp_z =      2.000
hyp_v =      0.200

recurrence times for the different species:
ions       :   22.95490
protons    :   72.58975
N for parallel boundary condition: 1
Electrons are adiabatic, beta is set to zero
double precision computation
parallel direction: 4th order Arakawa scheme
=========== hyper diffusion ============
hyp_z =      2.000
hyp_v =      0.200

recurrence times for the different species:
ions       :   22.95490
protons    :   72.58975
Electrons are adiabatic, beta is set to zero
*************************************************************
Starting eigenvalue computation with PETSC-3.7.7/SLEPc-3.7.4
*************************************************************
*************************************************************
Starting eigenvalue computation with PETSC-3.7.7/SLEPc-3.7.4
*************************************************************

initialization: alm                 
  all kx, ky modes equally excited,
  jacobian to the power of  2.000E+00 in z


initialization: alm                 
  all kx, ky modes equally excited,
  jacobian to the power of  2.000E+00 in z
EV solver/preconditioner: jd / asm
using    0 test vectors with length    253952, i.e.    0.00MB
precision:   0.1000E-03
Time to build explicit L_g:     2.11

--------------------------------------------------------------

Creating explicit representation of the linear operator without the field part
Rank of the linear operator:   253952
Memory requirements:    25.19 MB per core
--------------------------------------------------------------


EV solver/preconditioner: jd / asm
using    0 test vectors with length    253952, i.e.    0.00MB
precision:   0.1000E-03
--------------------------------------------------------------

Creating explicit representation of the linear operator without the field part
Rank of the linear operator:   253952
Memory requirements:    25.19 MB per core
using additive Schwarz preconditioning with   8 blocks per processor
Time to build explicit L_g:     1.83

--------------------------------------------------------------

using additive Schwarz preconditioning with   8 blocks per processor
Time to build explicit L_g:     1.65

--------------------------------------------------------------

Time to build explicit L_g:     1.66

--------------------------------------------------------------

using additive Schwarz preconditioning with   8 blocks per processor
using additive Schwarz preconditioning with   8 blocks per processor
Time to build explicit L_g:     1.88

--------------------------------------------------------------

using additive Schwarz preconditioning with   8 blocks per processor
Time to build explicit L_g:     1.37

--------------------------------------------------------------

Time to build explicit L_g:     1.36

--------------------------------------------------------------

using additive Schwarz preconditioning with   8 blocks per processor
using additive Schwarz preconditioning with   8 blocks per processor
Time to build explicit L_g:     1.47

--------------------------------------------------------------

using additive Schwarz preconditioning with   8 blocks per processor
Time to build explicit L_g:     1.65

--------------------------------------------------------------

Time to build explicit L_g:     1.37

--------------------------------------------------------------

using additive Schwarz preconditioning with   8 blocks per processor
using additive Schwarz preconditioning with   8 blocks per processor
Time to build explicit L_g:     1.49

--------------------------------------------------------------

using additive Schwarz preconditioning with   8 blocks per processor
number of iterations:    10
calculated      0 eigenvectors
***NO EIGENVALUES CALCULATED***
time for eigenvalue computation:    25.543 sec
******************************************************
Time for GENE simulation:     34.493 sec
number of iterations:    10
calculated      0 eigenvectors
***NO EIGENVALUES CALCULATED***
time for eigenvalue computation:    26.097 sec
******************************************************
Time for GENE simulation:     35.504 sec
number of iterations:    10
calculated      0 eigenvectors
***NO EIGENVALUES CALCULATED***
time for eigenvalue computation:    25.969 sec
******************************************************
Time for GENE simulation:     36.744 sec
number of iterations:    10
calculated      0 eigenvectors
***NO EIGENVALUES CALCULATED***
number of iterations:    10
calculated      0 eigenvectors
***NO EIGENVALUES CALCULATED***
number of iterations:    10
calculated      0 eigenvectors
***NO EIGENVALUES CALCULATED***
time for eigenvalue computation:    27.185 sec
******************************************************
Time for GENE simulation:     36.977 sec
time for eigenvalue computation:    27.125 sec
******************************************************
Time for GENE simulation:     37.004 sec
time for eigenvalue computation:    27.015 sec
******************************************************
Time for GENE simulation:     37.022 sec
number of iterations:    10
calculated      0 eigenvectors
***NO EIGENVALUES CALCULATED***
time for eigenvalue computation:    27.771 sec
******************************************************
Time for GENE simulation:     37.073 sec
number of iterations:    10
calculated      0 eigenvectors
***NO EIGENVALUES CALCULATED***
time for eigenvalue computation:    26.764 sec
******************************************************
Time for GENE simulation:     37.194 sec
number of iterations:    10
calculated      0 eigenvectors
***NO EIGENVALUES CALCULATED***
time for eigenvalue computation:    27.125 sec
******************************************************
Time for GENE simulation:     37.444 sec
number of iterations:    10
calculated      0 eigenvectors
***NO EIGENVALUES CALCULATED***
time for eigenvalue computation:    26.643 sec
******************************************************
Time for GENE simulation:     37.529 sec
number of iterations:    10
calculated      0 eigenvectors
***NO EIGENVALUES CALCULATED***
time for eigenvalue computation:    26.707 sec
******************************************************
Time for GENE simulation:     37.605 sec
number of iterations:    10
calculated      0 eigenvectors
***NO EIGENVALUES CALCULATED***
time for eigenvalue computation:    26.747 sec
******************************************************
Time for GENE simulation:     37.628 sec
Total wallclock time for GENE:     37.632 sec
Percentage of idle time at the end of the scan due to load imbalancing:      2.061 %
gmake[1]: Leaving directory '/home/stirkas/Workspace/GENE/genecode/prob05'
