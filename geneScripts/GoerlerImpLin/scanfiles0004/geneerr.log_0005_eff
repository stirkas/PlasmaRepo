using GENE executable in bin /home/stirkas/Workspace/GENE/genecode/finalResults
gmake -f ../makefiles/rules.mk run
gmake[1]: Entering directory '/home/stirkas/Workspace/GENE/genecode/finalResults'
Calling GENE with 16 MPI and 1 OpenMP thread(s)
 MPI_THREAD_MULTIPLE initialized.

*****************************************************
***** This is GENE 11 (release 1.8 - patch 1) *******
***** GIT branch hash: 99cb228                *******
***** GIT master hash:                        *******
*****************************************************

WARNING: no dpdx_term treatment defined!!
         Setting dpdx_term = 'gradB_eq_curv' ... 
no which_ev given in parameters, using jd
WARNING: no dpdx_term treatment defined!!
         Setting dpdx_term = 'gradB_eq_curv' ... 
WARNING: no dpdx_term treatment defined!!
         Setting dpdx_term = 'gradB_eq_curv' ... 
WARNING: no dpdx_term treatment defined!!
         Setting dpdx_term = 'gradB_eq_curv' ... 
no which_ev given in parameters, using jd
decreasing nx0 by 1 to have all kx physical for full matrix computations
WARNING: no dpdx_term treatment defined!!
         Setting dpdx_term = 'gradB_eq_curv' ... 
no which_ev given in parameters, using jd
decreasing nx0 by 1 to have all kx physical for full matrix computations
WARNING: no dpdx_term treatment defined!!
         Setting dpdx_term = 'gradB_eq_curv' ... 
no which_ev given in parameters, using jd
decreasing nx0 by 1 to have all kx physical for full matrix computations
WARNING: no dpdx_term treatment defined!!
         Setting dpdx_term = 'gradB_eq_curv' ... 
no which_ev given in parameters, using jd
decreasing nx0 by 1 to have all kx physical for full matrix computations
WARNING: no dpdx_term treatment defined!!
         Setting dpdx_term = 'gradB_eq_curv' ... 
no which_ev given in parameters, using jd
decreasing nx0 by 1 to have all kx physical for full matrix computations
WARNING: no dpdx_term treatment defined!!
         Setting dpdx_term = 'gradB_eq_curv' ... 
no which_ev given in parameters, using jd
decreasing nx0 by 1 to have all kx physical for full matrix computations
WARNING: no dpdx_term treatment defined!!
         Setting dpdx_term = 'gradB_eq_curv' ... 
no which_ev given in parameters, using jd
decreasing nx0 by 1 to have all kx physical for full matrix computations
WARNING: no dpdx_term treatment defined!!
         Setting dpdx_term = 'gradB_eq_curv' ... 
no which_ev given in parameters, using jd
decreasing nx0 by 1 to have all kx physical for full matrix computations
WARNING: no dpdx_term treatment defined!!
         Setting dpdx_term = 'gradB_eq_curv' ... 
no which_ev given in parameters, using jd
decreasing nx0 by 1 to have all kx physical for full matrix computations
WARNING: no dpdx_term treatment defined!!
         Setting dpdx_term = 'gradB_eq_curv' ... 
no which_ev given in parameters, using jd
decreasing nx0 by 1 to have all kx physical for full matrix computations
WARNING: no dpdx_term treatment defined!!
         Setting dpdx_term = 'gradB_eq_curv' ... 
no which_ev given in parameters, using jd
decreasing nx0 by 1 to have all kx physical for full matrix computations
WARNING: no dpdx_term treatment defined!!
         Setting dpdx_term = 'gradB_eq_curv' ... 
no which_ev given in parameters, using jd
decreasing nx0 by 1 to have all kx physical for full matrix computations
WARNING: no dpdx_term treatment defined!!
decreasing nx0 by 1 to have all kx physical for full matrix computations
no which_ev given in parameters, using jd
decreasing nx0 by 1 to have all kx physical for full matrix computations
no which_ev given in parameters, using jd
decreasing nx0 by 1 to have all kx physical for full matrix computations
         Setting dpdx_term = 'gradB_eq_curv' ... 
no which_ev given in parameters, using jd
decreasing nx0 by 1 to have all kx physical for full matrix computations
Using a maximum of  1300.00 MB per core.
Using a maximum of  1300.00 MB per core.
Using a maximum of  1300.00 MB per core.
Using a maximum of  1300.00 MB per core.
Using a maximum of  1300.00 MB per core.
Using a maximum of  1300.00 MB per core.
Using a maximum of  1300.00 MB per core.
Using a maximum of  1300.00 MB per core.
Using a maximum of  1300.00 MB per core.
Using a maximum of  1300.00 MB per core.
Using a maximum of  1300.00 MB per core.
Using a maximum of  1300.00 MB per core.
Using a maximum of  1300.00 MB per core.
Using a maximum of  1300.00 MB per core.
Using a maximum of  1300.00 MB per core.
Using a maximum of  1300.00 MB per core.
N for parallel boundary condition: 1
double precision computation
parallel direction: 4th order Arakawa scheme
=========== hyper diffusion ============
hyp_z =      2.000
hyp_v =      0.200

recurrence times for the different species:
ions       :   22.95490
electrons  :    0.53570
Kinetic electrons and ions
*************************************************************
Starting eigenvalue computation with PETSC-3.7.7/SLEPc-3.7.4
*************************************************************

initialization: alm                 
  all kx, ky modes equally excited,
  jacobian to the power of  2.000E+00 in z

EV solver/preconditioner: jd / asm
using    0 test vectors with length    253952, i.e.    0.00MB
precision:   0.1000E-03
--------------------------------------------------------------

Creating explicit representation of the linear operator without the field part
Rank of the linear operator:   253952
Memory requirements:    50.38 MB per core
N for parallel boundary condition: 1
double precision computation
parallel direction: 4th order Arakawa scheme
=========== hyper diffusion ============
hyp_z =      2.000
hyp_v =      0.200

recurrence times for the different species:
ions       :   22.95490
electrons  :    0.53570
Kinetic electrons and ions
N for parallel boundary condition: 1
double precision computation
parallel direction: 4th order Arakawa scheme
=========== hyper diffusion ============
hyp_z =      2.000
hyp_v =      0.200

recurrence times for the different species:
ions       :   22.95490
electrons  :    0.53570
N for parallel boundary condition: 1
Kinetic electrons and ions
double precision computation
parallel direction: 4th order Arakawa scheme
=========== hyper diffusion ============
hyp_z =      2.000
hyp_v =      0.200

recurrence times for the different species:
ions       :   22.95490
electrons  :    0.53570
*************************************************************
Starting eigenvalue computation with PETSC-3.7.7/SLEPc-3.7.4
*************************************************************
Kinetic electrons and ions
N for parallel boundary condition: 1
double precision computation
parallel direction: 4th order Arakawa scheme
=========== hyper diffusion ============
hyp_z =      2.000
hyp_v =      0.200

recurrence times for the different species:
ions       :   22.95490
electrons  :    0.53570
Kinetic electrons and ions
N for parallel boundary condition: 1

initialization: alm                 
  all kx, ky modes equally excited,
  jacobian to the power of  2.000E+00 in z

double precision computation
parallel direction: 4th order Arakawa scheme
=========== hyper diffusion ============
hyp_z =      2.000
hyp_v =      0.200

recurrence times for the different species:
ions       :   22.95490
electrons  :    0.53570
Kinetic electrons and ions
*************************************************************
Starting eigenvalue computation with PETSC-3.7.7/SLEPc-3.7.4
*************************************************************
EV solver/preconditioner: jd / asm
using    0 test vectors with length    253952, i.e.    0.00MB
precision:   0.1000E-03
--------------------------------------------------------------

Creating explicit representation of the linear operator without the field part
Rank of the linear operator:   253952
Memory requirements:    50.38 MB per core
*************************************************************
Starting eigenvalue computation with PETSC-3.7.7/SLEPc-3.7.4
*************************************************************
*************************************************************
Starting eigenvalue computation with PETSC-3.7.7/SLEPc-3.7.4
*************************************************************

initialization: alm                 
  all kx, ky modes equally excited,
  jacobian to the power of  2.000E+00 in z


initialization: alm                 
  all kx, ky modes equally excited,
  jacobian to the power of  2.000E+00 in z

initialization: alm                 
  all kx, ky modes equally excited,
  jacobian to the power of  2.000E+00 in z
EV solver/preconditioner: jd / asm
using    0 test vectors with length    253952, i.e.    0.00MB
precision:   0.1000E-03
--------------------------------------------------------------

Creating explicit representation of the linear operator without the field part
Rank of the linear operator:   253952
Memory requirements:    50.38 MB per core


*************************************************************
Starting eigenvalue computation with PETSC-3.7.7/SLEPc-3.7.4
*************************************************************
EV solver/preconditioner: jd / asm
using    0 test vectors with length    253952, i.e.    0.00MB
precision:   0.1000E-03
--------------------------------------------------------------

Creating explicit representation of the linear operator without the field part
Rank of the linear operator:   253952
Memory requirements:    50.38 MB per core
N for parallel boundary condition: 1
EV solver/preconditioner: jd / asm
using    0 test vectors with length    253952, i.e.    0.00MB
precision:   0.1000E-03
--------------------------------------------------------------

Creating explicit representation of the linear operator without the field part
Rank of the linear operator:   253952
Memory requirements:    50.38 MB per core
double precision computation
parallel direction: 4th order Arakawa scheme
=========== hyper diffusion ============
hyp_z =      2.000
hyp_v =      0.200

recurrence times for the different species:
ions       :   22.95490
electrons  :    0.53570
Kinetic electrons and ions

initialization: alm                 
  all kx, ky modes equally excited,
  jacobian to the power of  2.000E+00 in z

EV solver/preconditioner: jd / asm
using    0 test vectors with length    253952, i.e.    0.00MB
precision:   0.1000E-03
--------------------------------------------------------------

Creating explicit representation of the linear operator without the field part
Rank of the linear operator:   253952
Memory requirements:    50.38 MB per core
*************************************************************
Starting eigenvalue computation with PETSC-3.7.7/SLEPc-3.7.4
*************************************************************
N for parallel boundary condition: 1
double precision computation
parallel direction: 4th order Arakawa scheme
=========== hyper diffusion ============
hyp_z =      2.000
hyp_v =      0.200

recurrence times for the different species:
ions       :   22.95490
electrons  :    0.53570
Kinetic electrons and ions

initialization: alm                 
  all kx, ky modes equally excited,
  jacobian to the power of  2.000E+00 in z

N for parallel boundary condition: 1
double precision computation
parallel direction: 4th order Arakawa scheme
=========== hyper diffusion ============
hyp_z =      2.000
hyp_v =      0.200

recurrence times for the different species:
ions       :   22.95490
electrons  :    0.53570
Kinetic electrons and ions
EV solver/preconditioner: jd / asm
using    0 test vectors with length    253952, i.e.    0.00MB
precision:   0.1000E-03
--------------------------------------------------------------

Creating explicit representation of the linear operator without the field part
Rank of the linear operator:   253952
Memory requirements:    50.38 MB per core
*************************************************************
Starting eigenvalue computation with PETSC-3.7.7/SLEPc-3.7.4
*************************************************************
*************************************************************
Starting eigenvalue computation with PETSC-3.7.7/SLEPc-3.7.4
*************************************************************

initialization: alm                 
  all kx, ky modes equally excited,
  jacobian to the power of  2.000E+00 in z


initialization: alm                 
  all kx, ky modes equally excited,
  jacobian to the power of  2.000E+00 in z
EV solver/preconditioner: jd / asm
using    0 test vectors with length    253952, i.e.    0.00MB
precision:   0.1000E-03
--------------------------------------------------------------

Creating explicit representation of the linear operator without the field part
Rank of the linear operator:   253952
Memory requirements:    50.38 MB per core

EV solver/preconditioner: jd / asm
using    0 test vectors with length    253952, i.e.    0.00MB
precision:   0.1000E-03
N for parallel boundary condition: 1
--------------------------------------------------------------

Creating explicit representation of the linear operator without the field part
Rank of the linear operator:   253952
Memory requirements:    50.38 MB per core
double precision computation
parallel direction: 4th order Arakawa scheme
=========== hyper diffusion ============
hyp_z =      2.000
hyp_v =      0.200

recurrence times for the different species:
ions       :   22.95490
electrons  :    0.53570
Kinetic electrons and ions
N for parallel boundary condition: 1
double precision computation
parallel direction: 4th order Arakawa scheme
=========== hyper diffusion ============
hyp_z =      2.000
hyp_v =      0.200

recurrence times for the different species:
ions       :   22.95490
electrons  :    0.53570
Kinetic electrons and ions
*************************************************************
Starting eigenvalue computation with PETSC-3.7.7/SLEPc-3.7.4
*************************************************************

initialization: alm                 
  all kx, ky modes equally excited,
  jacobian to the power of  2.000E+00 in z

N for parallel boundary condition: 1
*************************************************************
Starting eigenvalue computation with PETSC-3.7.7/SLEPc-3.7.4
*************************************************************
EV solver/preconditioner: jd / asm
using    0 test vectors with length    253952, i.e.    0.00MB
precision:   0.1000E-03
--------------------------------------------------------------

Creating explicit representation of the linear operator without the field part
Rank of the linear operator:   253952
Memory requirements:    50.38 MB per core
double precision computation
parallel direction: 4th order Arakawa scheme
=========== hyper diffusion ============
hyp_z =      2.000
hyp_v =      0.200

recurrence times for the different species:
ions       :   22.95490
electrons  :    0.53570
Kinetic electrons and ions

initialization: alm                 
  all kx, ky modes equally excited,
  jacobian to the power of  2.000E+00 in z
N for parallel boundary condition: 1

double precision computation
parallel direction: 4th order Arakawa scheme
=========== hyper diffusion ============
hyp_z =      2.000
hyp_v =      0.200

recurrence times for the different species:
ions       :   22.95490
electrons  :    0.53570
*************************************************************
Starting eigenvalue computation with PETSC-3.7.7/SLEPc-3.7.4
*************************************************************
Kinetic electrons and ions
EV solver/preconditioner: jd / asm
using    0 test vectors with length    253952, i.e.    0.00MB
precision:   0.1000E-03
--------------------------------------------------------------

Creating explicit representation of the linear operator without the field part
Rank of the linear operator:   253952
Memory requirements:    50.38 MB per core
N for parallel boundary condition: 1
double precision computation
parallel direction: 4th order Arakawa scheme
=========== hyper diffusion ============
hyp_z =      2.000
hyp_v =      0.200

recurrence times for the different species:
ions       :   22.95490
electrons  :    0.53570
Kinetic electrons and ions

initialization: alm                 
  all kx, ky modes equally excited,
  jacobian to the power of  2.000E+00 in z
*************************************************************
Starting eigenvalue computation with PETSC-3.7.7/SLEPc-3.7.4
*************************************************************

EV solver/preconditioner: jd / asm
using    0 test vectors with length    253952, i.e.    0.00MB
precision:   0.1000E-03
--------------------------------------------------------------

Creating explicit representation of the linear operator without the field part
Rank of the linear operator:   253952
Memory requirements:    50.38 MB per core
N for parallel boundary condition: 1
double precision computation
parallel direction: 4th order Arakawa scheme
=========== hyper diffusion ============
hyp_z =      2.000
hyp_v =      0.200

recurrence times for the different species:
ions       :   22.95490
electrons  :    0.53570
Kinetic electrons and ions
*************************************************************
Starting eigenvalue computation with PETSC-3.7.7/SLEPc-3.7.4
*************************************************************

initialization: alm                 
  all kx, ky modes equally excited,
  jacobian to the power of  2.000E+00 in z
N for parallel boundary condition: 1

double precision computation
parallel direction: 4th order Arakawa scheme
=========== hyper diffusion ============
hyp_z =      2.000
hyp_v =      0.200

recurrence times for the different species:
ions       :   22.95490
electrons  :    0.53570
Kinetic electrons and ions

initialization: alm                 
  all kx, ky modes equally excited,
  jacobian to the power of  2.000E+00 in z
EV solver/preconditioner: jd / asm
using    0 test vectors with length    253952, i.e.    0.00MB
precision:   0.1000E-03
*************************************************************
Starting eigenvalue computation with PETSC-3.7.7/SLEPc-3.7.4
*************************************************************
--------------------------------------------------------------

Creating explicit representation of the linear operator without the field part
Rank of the linear operator:   253952
Memory requirements:    50.38 MB per core

EV solver/preconditioner: jd / asm
using    0 test vectors with length    253952, i.e.    0.00MB
precision:   0.1000E-03
--------------------------------------------------------------

Creating explicit representation of the linear operator without the field part
Rank of the linear operator:   253952
Memory requirements:    50.38 MB per core

initialization: alm                 
  all kx, ky modes equally excited,
  jacobian to the power of  2.000E+00 in z
*************************************************************
Starting eigenvalue computation with PETSC-3.7.7/SLEPc-3.7.4
*************************************************************

EV solver/preconditioner: jd / asm
using    0 test vectors with length    253952, i.e.    0.00MB
precision:   0.1000E-03
--------------------------------------------------------------

Creating explicit representation of the linear operator without the field part
Rank of the linear operator:   253952
Memory requirements:    50.38 MB per core

initialization: alm                 
  all kx, ky modes equally excited,
  jacobian to the power of  2.000E+00 in z

EV solver/preconditioner: jd / asm
using    0 test vectors with length    253952, i.e.    0.00MB
precision:   0.1000E-03
--------------------------------------------------------------

Creating explicit representation of the linear operator without the field part
Rank of the linear operator:   253952
Memory requirements:    50.38 MB per core
Time to build explicit L_g:     2.60

--------------------------------------------------------------

using additive Schwarz preconditioning with  16 blocks per processor
Time to build explicit L_g:     2.19

--------------------------------------------------------------

Time to build explicit L_g:     2.21

--------------------------------------------------------------

Time to build explicit L_g:     2.28

--------------------------------------------------------------

Time to build explicit L_g:     2.19

--------------------------------------------------------------

Time to build explicit L_g:     2.27

--------------------------------------------------------------

Time to build explicit L_g:     2.29

--------------------------------------------------------------

Time to build explicit L_g:     2.17

--------------------------------------------------------------

Time to build explicit L_g:     2.16

--------------------------------------------------------------

Time to build explicit L_g:     2.25

--------------------------------------------------------------

Time to build explicit L_g:     2.27

--------------------------------------------------------------

using additive Schwarz preconditioning with  16 blocks per processor
Time to build explicit L_g:     2.24

--------------------------------------------------------------

Time to build explicit L_g:     2.23

--------------------------------------------------------------

Time to build explicit L_g:     2.23

--------------------------------------------------------------

using additive Schwarz preconditioning with  16 blocks per processor
Time to build explicit L_g:     2.24

--------------------------------------------------------------

using additive Schwarz preconditioning with  16 blocks per processor
using additive Schwarz preconditioning with  16 blocks per processor
using additive Schwarz preconditioning with  16 blocks per processor
Time to build explicit L_g:     2.32

--------------------------------------------------------------

using additive Schwarz preconditioning with  16 blocks per processor
using additive Schwarz preconditioning with  16 blocks per processor
using additive Schwarz preconditioning with  16 blocks per processor
using additive Schwarz preconditioning with  16 blocks per processor
using additive Schwarz preconditioning with  16 blocks per processor
using additive Schwarz preconditioning with  16 blocks per processor
using additive Schwarz preconditioning with  16 blocks per processor
using additive Schwarz preconditioning with  16 blocks per processor
using additive Schwarz preconditioning with  16 blocks per processor
using additive Schwarz preconditioning with  16 blocks per processor
number of iterations:    10
calculated      0 eigenvectors
***NO EIGENVALUES CALCULATED***
time for eigenvalue computation:    65.309 sec
******************************************************
Time for GENE simulation:     78.648 sec
number of iterations:    10
calculated      0 eigenvectors
***NO EIGENVALUES CALCULATED***
number of iterations:    10
calculated      0 eigenvectors
***NO EIGENVALUES CALCULATED***
time for eigenvalue computation:    65.579 sec
******************************************************
Time for GENE simulation:     79.254 sec
time for eigenvalue computation:    65.391 sec
******************************************************
Time for GENE simulation:     79.260 sec
number of iterations:    10
calculated      0 eigenvectors
***NO EIGENVALUES CALCULATED***
number of iterations:    10
calculated      0 eigenvectors
***NO EIGENVALUES CALCULATED***
time for eigenvalue computation:    65.828 sec
******************************************************
Time for GENE simulation:     79.694 sec
number of iterations:    10
calculated      0 eigenvectors
***NO EIGENVALUES CALCULATED***
time for eigenvalue computation:    65.902 sec
******************************************************
Time for GENE simulation:     79.728 sec
number of iterations:    10
calculated      0 eigenvectors
***NO EIGENVALUES CALCULATED***
time for eigenvalue computation:    66.092 sec
******************************************************
Time for GENE simulation:     79.757 sec
number of iterations:    10
calculated      0 eigenvectors
***NO EIGENVALUES CALCULATED***
time for eigenvalue computation:    65.886 sec
******************************************************
Time for GENE simulation:     79.793 sec
number of iterations:    10
calculated      0 eigenvectors
***NO EIGENVALUES CALCULATED***
number of iterations:    10
calculated      0 eigenvectors
***NO EIGENVALUES CALCULATED***
number of iterations:    10
calculated      0 eigenvectors
***NO EIGENVALUES CALCULATED***
number of iterations:    10
calculated      0 eigenvectors
***NO EIGENVALUES CALCULATED***
number of iterations:    10
calculated      0 eigenvectors
***NO EIGENVALUES CALCULATED***
time for eigenvalue computation:    65.773 sec
******************************************************
Time for GENE simulation:     79.912 sec
time for eigenvalue computation:    65.931 sec
******************************************************
Time for GENE simulation:     79.923 sec
time for eigenvalue computation:    66.058 sec
******************************************************
Time for GENE simulation:     79.941 sec
number of iterations:    10
calculated      0 eigenvectors
***NO EIGENVALUES CALCULATED***
time for eigenvalue computation:    65.988 sec
******************************************************
Time for GENE simulation:     79.952 sec
time for eigenvalue computation:    66.119 sec
******************************************************
Time for GENE simulation:     79.958 sec
time for eigenvalue computation:    66.133 sec
******************************************************
Time for GENE simulation:     79.968 sec
number of iterations:    10
calculated      0 eigenvectors
***NO EIGENVALUES CALCULATED***
number of iterations:    10
calculated      0 eigenvectors
***NO EIGENVALUES CALCULATED***
time for eigenvalue computation:    66.168 sec
******************************************************
Time for GENE simulation:     79.983 sec
time for eigenvalue computation:    65.919 sec
******************************************************
Time for GENE simulation:     80.003 sec
time for eigenvalue computation:    66.101 sec
******************************************************
Time for GENE simulation:     80.005 sec
Total wallclock time for GENE:     80.009 sec
Percentage of idle time at the end of the scan due to load imbalancing:      0.336 %
gmake[1]: Leaving directory '/home/stirkas/Workspace/GENE/genecode/finalResults'
